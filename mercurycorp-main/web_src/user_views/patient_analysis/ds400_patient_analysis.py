# -*- coding: utf-8 -*-
"""ds400_patient_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16B2j-LnFVIHJ3FCdWS62Ai-7EvMiX97P
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#!/bin/bash
!kaggle datasets download prasad22/healthcare-dataset

kaggle_original = pd.read_csv('healthcare-dataset.zip')

# Later use
#synthea_v1 = pd.read_csv('https://mitre.box.com/shared/static/s9m4itxxzbw7q9gy68wf84foev3x1t6y.gz', compression='gzip')
#synthea_v2 = pd.read_csv('https://mitre.box.com/shared/static/3bo45m48ocpzp8fc0tp005vax7l93xji.gz', compression='gzip')

kaggle_original.head()

kaggle_original.info()

kaggle_original.describe()

kaggle_original.isnull().sum()

# Handle Data Types and Values
kaggle_original['Name'] = kaggle_original['Name'].str.lower()
kaggle_original['Date of Admission'] = pd.to_datetime(kaggle_original['Date of Admission'])
kaggle_original['Discharge Date'] = pd.to_datetime(kaggle_original['Discharge Date'])
kaggle_original['Billing Amount'] = kaggle_original['Billing Amount'].round(2)

kaggle_original.drop_duplicates(inplace=True)

kaggle_original.head()

kaggle_original.describe()

for col in kaggle_original.columns:
    mode_values = kaggle_original[col].mode().iloc[0]
    print(f"Most frequent value in column '{col}': {mode_values}")

#plotting figures for both age and medical condition
plt.figure(figsize=(16, 8))
sns.countplot(x='Age', hue='Medical Condition', data=kaggle_original)
plt.title('Age vs. Medical Condition')
plt.xlabel('Age')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

geriatric_data = kaggle_original[kaggle_original['Age'] >= 65]

#plotting figures for both age and medical condition
plt.figure(figsize=(16, 8))
sns.countplot(x='Age', hue='Medical Condition', data=geriatric_data)
plt.title('Age vs. Medical Condition')
plt.xlabel('Age')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(y='Medical Condition', data=kaggle_original)
plt.title('Value Counts of Medical Conditions')
plt.xlabel('Count')
plt.ylabel('Medical Condition')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(y='Admission Type', data=kaggle_original)
plt.title('Value Counts of Admission Type')
plt.xlabel('Count')
plt.ylabel('Admission Type')
plt.tight_layout()
plt.show()

condition_counts = geriatric_data['Medical Condition'].value_counts()
condition_counts

sns.set(style="whitegrid")
plt.figure(figsize=(12, 6))

# Age
plt.subplot(2, 3, 1)
sns.histplot(geriatric_data["Age"], bins=30, kde=True, color="blue")
plt.title("Age Distribution")

# Gender
plt.subplot(2, 3, 2)
sns.countplot(x=geriatric_data["Gender"], palette="coolwarm")
plt.title("Gender Distribution")

# Blood type
plt.subplot(2, 3, 3)
sns.countplot(x=geriatric_data["Blood Type"], palette="muted")
plt.title("Blood Type Distribution")

# Medical Condition
plt.figure(figsize=(10, 5))
sns.countplot(y=geriatric_data["Medical Condition"], palette="pastel", order=geriatric_data["Medical Condition"].value_counts().index)
plt.title("Medical Condition Distribution")
plt.xlabel("Count")

# Billing amt
plt.figure(figsize=(8, 5))
sns.histplot(geriatric_data["Billing Amount"], bins=30, kde=True, color="green")
plt.title("Billing Amount Distribution")
plt.xlabel("Billing Amount")

# Admissions Over Time
plt.figure(figsize=(10, 5))
geriatric_data["Date of Admission"].value_counts().sort_index().plot()
plt.title("Admissions Over Time")
plt.xlabel("Date of Admission")
plt.ylabel("Number of Patients")

plt.show()

"""#Modeling for medical condition"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

geriatric_data.info()

# length of stay in place of datetime variables
geriatric_data['Length of Stay'] = (geriatric_data['Discharge Date'] - geriatric_data['Date of Admission']).dt.days

# dropping columns with no correlation
geriatric_data_encoded = geriatric_data.drop(columns=['Name', 'Room Number', 'Hospital', 'Doctor', 'Date of Admission', 'Discharge Date'])

# Encode categorical features
categorical_features = ['Gender', 'Insurance Provider', 'Admission Type', 'Blood Type', 'Medication', 'Test Results']

encoder = OneHotEncoder(sparse_output=False, drop='first')
encoded_features = encoder.fit_transform(geriatric_data_encoded[categorical_features])

encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features), index=geriatric_data_encoded.index)

# Target - Medical Condition
label_encoder = LabelEncoder()
geriatric_data_encoded['Medical Condition Encoded'] = label_encoder.fit_transform(geriatric_data_encoded['Medical Condition'])

geraitric_data_final = pd.concat([geriatric_data_encoded.drop(columns=categorical_features + ['Medical Condition']), encoded_df], axis=1)

# Features and target
X = geraitric_data_final.drop(columns=['Medical Condition Encoded'])  # Predictors
y = geraitric_data_final['Medical Condition Encoded']  # Encoded target

# Scale numerical features
scaler = StandardScaler()
numerical_features = ['Age', 'Billing Amount', 'Length of Stay']
X[numerical_features] = scaler.fit_transform(X[numerical_features])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

model = LogisticRegression(solver='lbfgs', max_iter=2000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

predictions_decoded = label_encoder.inverse_transform(y_pred)

print(f'Accuracy: {accuracy}')
print(f'Predictions: {predictions_decoded}')

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

"""## Model is not the best for capturing the complexity of the data. cols that have been removed have no correlation with the patients medical condition. We will need a better model to predict medical condition. However, we can test below if we can predict the length of stay based off of medical condition - another costly subject."""

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Redoing the data splitting to run cell alone
geriatric_data['Length of Stay'] = (geriatric_data['Discharge Date'] - geriatric_data['Date of Admission']).dt.days

geraitric_data_encoded = geriatric_data.drop(columns=['Name', 'Room Number', 'Hospital', 'Doctor', 'Date of Admission', 'Discharge Date'])

categorical_features = ['Gender', 'Insurance Provider', 'Admission Type', 'Blood Type', 'Medication', 'Test Results']

encoder = OneHotEncoder(sparse_output=False, drop='first')
encoded_features = encoder.fit_transform(geraitric_data_encoded[categorical_features])

encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features), index=geraitric_data_encoded.index)

label_encoder = LabelEncoder()
geraitric_data_encoded['Medical Condition Encoded'] = label_encoder.fit_transform(geraitric_data_encoded['Medical Condition'])

geraitric_data_final = pd.concat([geraitric_data_encoded.drop(columns=categorical_features + ['Medical Condition']), encoded_df], axis=1)

X = geraitric_data_final.drop(columns=['Length of Stay'])
y = geraitric_data_final['Length of Stay']

scaler = StandardScaler()
numerical_features = ['Age', 'Billing Amount']
X[numerical_features] = scaler.fit_transform(X[numerical_features])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

# LR
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

# RF
rf_reg = RandomForestRegressor(n_estimators=100, random_state=101)
rf_reg.fit(X_train, y_train)

lin_pred = lin_reg.predict(X_test)
rf_pred = rf_reg.predict(X_test)

# Evaluate
lin_mae = mean_absolute_error(y_test, lin_pred)
lin_rmse = np.sqrt(mean_squared_error(y_test, lin_pred))
lin_r2 = r2_score(y_test, lin_pred)

rf_mae = mean_absolute_error(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
rf_r2 = r2_score(y_test, rf_pred)

print('Linear Regression Results:')
print(f'MAE: {lin_mae}, RMSE: {lin_rmse}, R²: {lin_r2}')

print('\nRandom Forest Regression Results:')
print(f'MAE: {rf_mae}, RMSE: {rf_rmse}, R²: {rf_r2}')

plt.figure(figsize=(10, 6))
sns.histplot(geriatric_data['Length of Stay'], bins=30, kde=True)
plt.title('Histogram of Length of Stay')
plt.xlabel('Length of Stay (days)')
plt.ylabel('Frequency')
plt.show()

"""## Overall, we can see that these models are too simple to get the complexity of the data, whether is be the medical condition or the length of stay--both targets being pretty evenly distributed. We can try using more complex models to predict the target of medical condition and length of stay.

# LSTM and GRU models
"""

#Importing the libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Data processing stage
geriatric_data['Length of Stay'] = (geriatric_data['Discharge Date'] - geriatric_data['Date of Admission']).dt.days

geraitric_data_encoded = geriatric_data.drop(columns=['Name', 'Room Number', 'Hospital', 'Doctor', 'Date of Admission', 'Discharge Date'])

categorical_features = ['Gender', 'Insurance Provider', 'Admission Type', 'Blood Type', 'Medication', 'Test Results']

encoder = OneHotEncoder(sparse_output=False, drop='first')
encoded_features = encoder.fit_transform(geraitric_data_encoded[categorical_features])

encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features), index=geraitric_data_encoded.index)

label_encoder = LabelEncoder()
geraitric_data_encoded['Medical Condition Encoded'] = label_encoder.fit_transform(geraitric_data_encoded['Medical Condition'])

geraitric_data_final = pd.concat([geraitric_data_encoded.drop(columns=categorical_features + ['Medical Condition']), encoded_df], axis=1)

# Length of stay first
X = geraitric_data_final.drop(columns=['Length of Stay'])
y = geraitric_data_final['Length of Stay']

scaler = StandardScaler()
numerical_features = ['Age', 'Billing Amount']
X[numerical_features] = scaler.fit_transform(X[numerical_features])

# LSTN needs shape as(samples, timesteps, features), we need to add the timestep
X = np.expand_dims(X.values, axis=1)

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

# LSTM Model
model = Sequential()
model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # Output layer for regression

# Compile
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# Predictions
y_pred = model.predict(X_test)

# Evaluate
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print('LSTM Regression Results:')
print(f'MAE: {mae}, RMSE: {rmse}, R²: {r2}')

# Using bi-directional LSTM?
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping

#Processing
geriatric_data['Length of Stay'] = (geriatric_data['Discharge Date'] - geriatric_data['Date of Admission']).dt.days

# Selecting features
features = ['Age', 'Billing Amount', 'Room Number']
target = 'Length of Stay'

# converting into arrays
X = geriatric_data[features].values
y = geriatric_data[target].values

# Reshape data for LSTM (samples, time steps, features)
X = X.reshape((X.shape[0], 1, X.shape[1]))

# Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build & Train Bidirectional LSTM Model
model = Sequential([
    Bidirectional(LSTM(100, activation="relu", return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])),
    LSTM(50, activation="relu"),
    Dense(1)
])

model.compile(optimizer="adam", loss="mse")
early_stop = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop])

# Plot training vs. validation loss
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss (MSE)")
plt.title("LSTM Model Training Loss")
plt.legend()
plt.show()

# Predictions
y_pred = model.predict(X_test)

plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred, alpha=0.5, color="blue")
plt.xlabel("Actual Length of Stay")
plt.ylabel("Predicted Length of Stay")
plt.title("LSTM Predictions vs Actual")
plt.show()

## Might be some underfitting? looks like the limited features of the data might be struggling

"""## We can contiue to GRU. Difference between both is that GRU is better with simple and fast data, whereas LSTM works is more complex data. This would be good to consider when using synthea records that are more complex."""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout

# Setting up again
geriatric_data['Length of Stay'] = (geriatric_data['Discharge Date'] - geriatric_data['Date of Admission']).dt.days

geraitric_data_encoded = geriatric_data.drop(columns=['Name', 'Room Number', 'Hospital', 'Doctor', 'Date of Admission', 'Discharge Date'])

categorical_features = ['Gender', 'Insurance Provider', 'Admission Type', 'Blood Type', 'Medication', 'Test Results']

encoder = OneHotEncoder(sparse_output=False, drop='first')
encoded_features = encoder.fit_transform(geraitric_data_encoded[categorical_features])

encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features), index=geraitric_data_encoded.index)

label_encoder = LabelEncoder()
geraitric_data_encoded['Medical Condition Encoded'] = label_encoder.fit_transform(geraitric_data_encoded['Medical Condition'])

geraitric_data_final = pd.concat([geraitric_data_encoded.drop(columns=categorical_features + ['Medical Condition']), encoded_df], axis=1)

# Still on length of stay
X = geraitric_data_final.drop(columns=['Length of Stay'])
y = geraitric_data_final['Length of Stay']

scaler = StandardScaler()
numerical_features = ['Age', 'Billing Amount']
X[numerical_features] = scaler.fit_transform(X[numerical_features])

# Reshape for GRU same as LSTM
X = np.expand_dims(X.values, axis=1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# GRU Model
model = Sequential()
model.add(GRU(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # Output layer for regression

# Compile
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# Predictions
y_pred = model.predict(X_test)

# Evaluate
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print('GRU Regression Results:')
print(f'MAE: {mae}, RMSE: {rmse}, R²: {r2}')

"""## As shown above, using these models in a basic way is not obtaining any results that are better than just predicting an average for the length of stay. Below i have used some more models that might capture the data better."""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Data processing
geriatric_data['Length of Stay'] = (geriatric_data['Discharge Date'] - geriatric_data['Date of Admission']).dt.days

geraitric_data_encoded = geriatric_data.drop(columns=['Name', 'Room Number', 'Hospital', 'Doctor', 'Date of Admission', 'Discharge Date'])

categorical_features = ['Gender', 'Insurance Provider', 'Admission Type', 'Blood Type', 'Medication', 'Test Results']

encoder = OneHotEncoder(sparse_output=False, drop='first')
encoded_features = encoder.fit_transform(geraitric_data_encoded[categorical_features])

encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features), index=geraitric_data_encoded.index)

label_encoder = LabelEncoder()
geraitric_data_encoded['Medical Condition Encoded'] = label_encoder.fit_transform(geraitric_data_encoded['Medical Condition'])

geraitric_data_final = pd.concat([geraitric_data_encoded.drop(columns=categorical_features + ['Medical Condition']), encoded_df], axis=1)

X = geraitric_data_final.drop(columns=['Length of Stay'])
y = geraitric_data_final['Length of Stay']

scaler = StandardScaler()
numerical_features = ['Age', 'Billing Amount']
X[numerical_features] = scaler.fit_transform(X[numerical_features])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

# DNN
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # Output layer for regression

# Compile
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# Predictins
y_pred = model.predict(X_test)

# Evaluate
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print('DNN Regression Results:')
print(f'MAE: {mae}, RMSE: {rmse}, R²: {r2}')

"""## These models not being able to accuratly predict the length of stay may suggest that there are no variables that are correlated in a significant way to the variable of the length of stay. Next, we will have another predictive model that will now use the target variable of medical condition.

# Modeling for target of Medical Condition
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Processing
geriatric_data['Length of Stay'] = (geriatric_data['Discharge Date'] - geriatric_data['Date of Admission']).dt.days

geraitric_data_encoded = geriatric_data.drop(columns=['Name', 'Room Number', 'Hospital', 'Doctor', 'Date of Admission', 'Discharge Date'])

categorical_features = ['Gender', 'Insurance Provider', 'Admission Type', 'Blood Type', 'Medication', 'Test Results']

encoder = OneHotEncoder(sparse_output=False, drop='first')
encoded_features = encoder.fit_transform(geraitric_data_encoded[categorical_features])

encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features), index=geraitric_data_encoded.index)

label_encoder = LabelEncoder()
geraitric_data_encoded['Medical Condition Encoded'] = label_encoder.fit_transform(geraitric_data_encoded['Medical Condition'])

geraitric_data_final = pd.concat([geraitric_data_encoded.drop(columns=categorical_features + ['Medical Condition']), encoded_df], axis=1)

# Features
X = geraitric_data_final.drop(columns=['Medical Condition Encoded'])  # Predictors
y = geraitric_data_final['Medical Condition Encoded']  # Target variable

# Scaling numerical
scaler = StandardScaler()
numerical_features = ['Age', 'Billing Amount']
X[numerical_features] = scaler.fit_transform(X[numerical_features])

# Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

# DNN for classification
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dense(len(label_encoder.classes_), activation='softmax'))

# Compile
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# Predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Evaluate
accuracy = accuracy_score(y_test, y_pred_classes)
report = classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_)

print('DNN Classification Results:')
print(f'Accuracy: {accuracy}')
print('\nClassification Report:\n')
print(report)

